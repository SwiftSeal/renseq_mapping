import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from Bio import SeqIO

experiment_name = config["experiment_name"]

def get_reference():
    return config["reference"]["fasta"]

def get_reads(wildcards):
    R1 = config[wildcards.source][wildcards.phenotype]["R1"]
    R2 = config[wildcards.source][wildcards.phenotype]["R2"]
    return R1, R2

def get_thresholds(wildcards):
    susceptible_min = config["allele_frequency"][wildcards.source]["susceptible"]["min"]
    susceptible_max = config["allele_frequency"][wildcards.source]["susceptible"]["max"]
    resistant_min = config["allele_frequency"][wildcards.source]["resistant"]["min"]
    resistant_max = config["allele_frequency"][wildcards.source]["resistant"]["max"]

    return susceptible_min, susceptible_max, resistant_min, resistant_max

if config["mode"] == "mapping":
    include: "rules/mapping.smk"
    rule all:
        input:
            expand("results/{experiment}/filtered/{source}.vcf", source = ["parent", "bulk"], experiment = experiment_name),
            expand("results/{experiment}/plots/{source}.png", source = ["parent", "bulk"], experiment = experiment_name),
            expand("results/{experiment}/result.tsv", experiment = experiment_name),
            expand("results/{experiment}/blast.txt", experiment = experiment_name)
elif config["mode"] == "standard":
    rule all:
        input:
            expand("results/{experiment}/filtered/{source}.vcf", source = ["parent", "bulk"], experiment = experiment_name),
            expand("results/{experiment}/plots/{source}.png", source = ["parent", "bulk"], experiment = experiment_name),
            expand("results/{experiment}/result.tsv", experiment = experiment_name)
elif config["mode"] == "report":
    rule all:
        input:
            expand("results/{experiment}/filtered/{source}.vcf", source = ["parent", "bulk"], experiment = experiment_name),
            expand("results/{experiment}/plots/{source}.png", source = ["parent", "bulk"], experiment = experiment_name),
            expand("results/{experiment}/result.tsv", experiment = experiment_name),
            # expand("results/{experiment}/result_repulsion.tsv", experiment = experiment_name),
            # expand("results/{experiment}/result_null.tsv", experiment = experiment_name),
            expand("results/{experiment}/reporting/blast_plot.png", experiment = experiment_name),
            expand("results/{experiment}/reporting/report.txt", experiment = experiment_name)
else:
    raise ValueError("Invalid mode specified in config file")

rule bowtie2_index:
    input:
        fasta = get_reference()
    output:
        expand("results/{experiment}/reference/reference.{i}.bt2", i = [1, 2, 3, 4, "rev.1", "rev.2"], experiment = experiment_name)
    params:
        index_directory = expand("results/{experiment}/reference/reference", experiment = experiment_name)
    conda:
        "envs/bowtie2.yaml"
    threads: 1
    resources:
        mem_mb = 8000,
        partition = "short"
    shell:
        """
        bowtie2-build {input.fasta} {params.index_directory}
        """

rule fastp:
    input:
        R1 = lambda wildcards: get_reads(wildcards)[0],
        R2 = lambda wildcards: get_reads(wildcards)[1]
    output:
        R1 = temp("results/{experiment}/fastp/{source}.{phenotype}.R1.fastq.gz"),
        R2 = temp("results/{experiment}/fastp/{source}.{phenotype}.R2.fastq.gz")

    conda:
        "envs/bowtie2.yaml"
    threads:
        4
    resources:
        mem_mb = 4000,
        partition = "short"
    shell:
        """
        fastp -i {input.R1} -I {input.R2} -o {output.R1} -O {output.R2} -h "results/{wildcards.experiment}/fastp/{wildcards.source}.{wildcards.phenotype}.html" -j "results/{wildcards.experiment}/fastp/{wildcards.source}.{wildcards.phenotype}.json"
        """

rule count_reads:
    input:
        R1 = "results/{experiment}/fastp/{source}.{phenotype}.R1.fastq.gz",
        R2 = "results/{experiment}/fastp/{source}.{phenotype}.R2.fastq.gz"
    output:
        counts = temp("results/{experiment}/reporting/read_counts/{source}.{phenotype}.readcounts.txt")
    resources:
        mem_mb = 1000,
        partition = "short"
    shell:
        """
        R1_count=$(echo $(zcat {input.R1} | wc -l)/4 | bc)
        R2_count=$(echo $(zcat {input.R2} | wc -l)/4 | bc)
        printf "R1\t$R1_count\n" > {output}
        printf "R2\t$R2_count\n" >> {output}
        """

rule bowtie2_align:
    input:
        R1 = "results/{experiment}/fastp/{source}.{phenotype}.R1.fastq.gz",
        R2 = "results/{experiment}/fastp/{source}.{phenotype}.R2.fastq.gz",
        index = "results/{experiment}/reference/reference.1.bt2"
    output:
        bam = temp("results/{experiment}/bowtie2_align/{source}.{phenotype}.bam")
    params:
        bowtie2_args = config["bowtie2_args"],
        index_directory = "results/{experiment}/reference/reference"
    conda:
        "envs/bowtie2.yaml"
    threads:
        16
    resources:
        mem_mb = 4000,
        partition = "short"
    shell:
        """
        bowtie2 -p {threads} {params.bowtie2_args} -x {params.index_directory} -1 {input.R1} -2 {input.R2} | samtools view -bS - > {output}
        """

rule sort_bam:
    input:
        "results/{experiment}/bowtie2_align/{source}.{phenotype}.bam"
    output:
        temp("results/{experiment}/bowtie2_align/{source}.{phenotype}.sorted.bam")
    conda:
        "envs/bowtie2.yaml"
    shell:
        """
        samtools sort -o {output} {input}
        """

rule mpileup:
    input:
        resistant = "results/{experiment}/bowtie2_align/{source}.resistant.sorted.bam",
        susceptible = "results/{experiment}/bowtie2_align/{source}.susceptible.sorted.bam",
        reference = get_reference()
    output:
        temp("results/{experiment}/mpileup/{source}.mpileup")
    conda:
        "envs/bowtie2.yaml"
    shell:
        """
        # mpileup builds an index at location of reference, so copy to tmpdir
        cp {input.reference} $TMPDIR/reference.fa
        samtools mpileup -f $TMPDIR/reference.fa {input.resistant} {input.susceptible} > {output}
        """

rule varscan:
    input:
        "results/{experiment}/mpileup/{source}.mpileup"
    output:
        "results/{experiment}/varscan/{source}.vcf"
    conda:
        "envs/bowtie2.yaml"
    shell:
        """
        varscan mpileup2snp {input} --output-vcf 1 --strand-filter 0 > {output}
        """

rule plot_variants_frequency:
    input:
        "results/{experiment}/varscan/{source}.vcf"
    output:
        report("results/{experiment}/plots/{source}.png")
    run:
        with open(input[0]) as infile:
            source = []
            frequency = []
            for line in infile:
                # ignore header lines
                if line.startswith("#"):
                    continue
                
                fields = line.split("\t")
                keys = fields[8].split(":")
                resistant = fields[9].split(":")
                susceptible = fields[10].split(":")
                
                # error and warn user if operating on wrong field
                if keys[6] != "FREQ":
                    raise ValueError("Expected FREQ field at position 7, got {} instead".format(keys[6]))

                if len(resistant) != len(keys) or len(susceptible) != len(keys):
                    continue
                
                # convert freq to float
                resistant_freq = float(resistant[6].rstrip("%"))
                susceptible_freq = float(susceptible[6].rstrip("%"))

                # add to dataframe
                source.append("resistant")
                frequency.append(resistant_freq)
                source.append("susceptible")
                frequency.append(susceptible_freq)

            # create dataframe
            df = pd.DataFrame({"source": source, "frequency": frequency})
            
            # plot
            sns.displot(df, x = "frequency", hue = "source", kind = "kde")
            plt.savefig(output[0])
    
rule filter_variants:
    input:
        "results/{experiment}/varscan/{source}.vcf"
    output:
        report("results/{experiment}/filtered/{source}.vcf")
    params:
        susceptible_min = lambda wildcards: get_thresholds(wildcards)[0],
        susceptible_max = lambda wildcards: get_thresholds(wildcards)[1],
        resistant_min = lambda wildcards: get_thresholds(wildcards)[2],
        resistant_max = lambda wildcards: get_thresholds(wildcards)[3]
    run:
        with open(input[0]) as infile, open(output[0], "w") as outfile:
            for line in infile:
                # ignore header lines
                if line.startswith("#"):
                    outfile.write(line)
                else:
                    fields = line.split("\t")
                    keys = fields[8].split(":")
                    resistant = fields[9].split(":")
                    susceptible = fields[10].split(":")

                    # error and warn user if operating on wrong field
                    if keys[6] != "FREQ":
                        raise ValueError("Expected FREQ field at position 7, got {} instead".format(keys[6]))

                    # ignore lines where resistant or susceptible is busted
                    if len(resistant) != len(keys) or len(susceptible) != len(keys):
                        continue
                    
                    # convert freq to float
                    resistant_freq = float(resistant[6].rstrip("%"))
                    susceptible_freq = float(susceptible[6].rstrip("%"))

                    # write out if freq is within bounds
                    if params.susceptible_min <= susceptible_freq <= params.susceptible_max and params.resistant_min <= resistant_freq <= params.resistant_max:
                        outfile.write(line)
                    elif 100 - params.susceptible_max <= susceptible_freq <= 100 - params.susceptible_min and 100 - params.resistant_max <= resistant_freq <= 100 - params.resistant_min:
                        outfile.write(line)

rule quantify_snps:
    input:
        unfiltered = "results/{experiment}/varscan/{source}.vcf",
        filtered = "results/{experiment}/filtered/{source}.vcf"
    output:
        unfiltered = temp("results/{experiment}/reporting/snp_counts/{source}.unfiltered.snpcounts.txt"),
        filtered = temp("results/{experiment}/reporting/snp_counts/{source}.filtered.snpcounts.txt")
    resources:
        mem_mb = 1000,
        partition = "short"
    shell:
        """
        cat {input.unfiltered} | grep -v "#" | wc -l > {output.unfiltered}
        cat {input.filtered} | grep -v "#" | wc -l > {output.filtered}
        """
                    
rule merge_variants:
    input:
        expand("results/{experiment}/filtered/{source}.vcf", source = ["parent", "bulk"], experiment = experiment_name)
    output:
        report("results/{experiment}/result.tsv")
    run:
        parent_df = pd.read_csv(input[0], sep = "\t", comment = "#", header = None)
        parent_df = parent_df[[0, 1]]
        parent_df.columns = ["chromosome", "position"]

        bulk_df = pd.read_csv(input[1], sep = "\t", comment = "#", header = None)
        bulk_df = bulk_df[[0, 1]]
        bulk_df.columns = ["chromosome", "position"]

        # add source column
        parent_df["source"] = "parent"
        bulk_df["source"] = "bulk"

        intersect = pd.merge(parent_df, bulk_df, how = "inner", on = ["chromosome", "position"])

        # write out
        intersect.to_csv(output[0], sep = "\t", index = False)

rule count_results:
    input:
        results = "results/{experiment}/result.tsv"
    output:
        results = temp("results/{experiment}/results_count.txt")
    resources:
        mem_mb = 1000,
        partition = "short"
    shell:
        """
        cat {input.results} | tail -n +2 | wc -l > {output.results}
        """

rule identify_SNPs_in_NLRs:
    input:
        nlr_bed = config["nlr_bed"],
        vcf = "results/{experiment}/result.tsv"
    output:
        snps = "results/{experiment}/interesting_snps.txt"
    params:
        flanking = config["distance_to_be_near_nlrs"]
    resources:
        mem_mb = 2000,
        partition = "short"
    shell:
        """
        python workflow/scripts/find_interesting_snps_biparental.py --input_SNPs {input.vcf} --input_NLRs {input.nlr_bed} --output {output.snps} --flanking {params.flanking}
        """

rule create_report:
    input:
        read_counts = expand("results/{experiment}/reporting/read_counts/{source}.{phenotype}.readcounts.txt", source = ["parent", "bulk"], experiment = experiment_name, phenotype = ["resistant", "susceptible"]),
        snp_counts = expand("results/{experiment}/reporting/snp_counts/{source}.{filtering}.snpcounts.txt", experiment = experiment_name, source = ["parent", "bulk"], filtering = ["filtered", "unfiltered"]),
        interesting_snps = "results/{experiment}/interesting_snps.txt",
        blast_results = "results/{experiment}/blast.txt",
        results_count = "results/{experiment}/results_count.txt"
    output:
        report = "results/{experiment}/reporting/report.txt"
    params:
        experiment_name = config["experiment_name"],
        R_parent_name = config["parent"]["resistant"]["name"],
        S_parent_name = config["parent"]["susceptible"]["name"],
        R_bulk_path = config["bulk"]["resistant"]["R1"],
        S_bulk_path = config["bulk"]["susceptible"]["R1"],
        reference_name = config["reference"]["name"],
        parent_S_min = config["allele_frequency"]["parent"]["susceptible"]["min"],
        parent_S_max = config["allele_frequency"]["parent"]["susceptible"]["max"],
        parent_R_min = config["allele_frequency"]["parent"]["resistant"]["min"],
        parent_R_max = config["allele_frequency"]["parent"]["resistant"]["max"],
        bulk_S_min = config["allele_frequency"]["bulk"]["susceptible"]["min"],
        bulk_S_max = config["allele_frequency"]["bulk"]["susceptible"]["max"],
        bulk_R_min = config["allele_frequency"]["bulk"]["resistant"]["min"],
        bulk_R_max = config["allele_frequency"]["bulk"]["resistant"]["max"]
    resources:
        mem_mb = 2000,
        partition = "short"
    shell:
        """
        python3 workflow/scripts/create_biparental_report.py --output {output.report} --read_counts {input.read_counts} --experiment_name {params.experiment_name} --R_parent_name {params.R_parent_name} --S_parent_name {params.S_parent_name}  --R_bulk_path {params.R_bulk_path} --S_bulk_path {params.S_bulk_path} --reference_name {params.reference_name} --snp_counts {input.snp_counts} --parent_S_min {params.parent_S_min} --parent_S_max {params.parent_S_max} --parent_R_min {params.parent_R_min} --parent_R_max {params.parent_R_max} --bulk_S_min {params.bulk_S_min} --bulk_S_max {params.bulk_S_max} --bulk_R_min {params.bulk_R_min} --bulk_R_max {params.bulk_R_max} --interesting_snps {input.interesting_snps} --blast_results {input.blast_results} --results_count {input.results_count}
        """

rule sizes:
    input:
        blast_genome = config["mapping_reference"]
    output:
        temp("results/{experiment}/genome_sizes.txt")
    conda:
        "envs/bioawk.yaml"
    resources:
        mem_mb = 1000,
        partition = "short"
    shell:
        """
        bioawk -c fastx '{{ print $name, length($seq) }}' {input.blast_genome} > {output}
        """

rule report_plot:
    input:
        blast_genome = config["mapping_reference"],
        blast_result = "results/{experiment}/blast.txt",
        genome_size = "results/{experiment}/genome_sizes.txt"
    output:
        plot = "results/{experiment}/reporting/blast_plot.png"
    params:
        experiment_name = config["experiment_name"]
    conda:
        "envs/plot.yaml"
    resources:
        mem_mb = 1000,
        partition = "short"
    shell:
        """
        Rscript --vanilla workflow/scripts/blast_plot.R {input.genome_size} {input.blast_result} {params.experiment_name} {output.plot}
        """

rule get_contigs:
    input:
        contigs = get_reference(),
        result = "results/{experiment}/result.tsv"
    output:
        filtered_contigs = "results/{experiment}/reference/filtered_reference.fasta"
    run:
        with open(input.contigs) as fasta, open(input.result) as result, open(output.filtered_contigs, "w") as filtered:
            # get contigs to keep
            contigs = set()
            for line in result:
                fields = line.split("\t")
                contigs.add(fields[0])
            
            # write out contigs
            for record in SeqIO.parse(fasta, "fasta"):
                if record.id in contigs:
                    SeqIO.write(record, filtered, "fasta")

rule blast:
    input:
        contigs = "results/{experiment}/reference/filtered_reference.fasta",
        reference = config["mapping_reference"]
    output:
        "results/{experiment}/blast.txt"
    conda:
        "envs/bowtie2.yaml"
    resources:
        mem_mb = 4000,
        partition = "short"
    shell:
        """
        blastn -query {input.contigs} -subject {input.reference} -outfmt 6 | sort -k1,1 -k12,12nr -k11,11n | sort -u -k1,1 --merge > {output}
        """
